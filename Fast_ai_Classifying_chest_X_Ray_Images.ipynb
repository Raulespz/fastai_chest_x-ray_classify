{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqw0TbPPLU/iA6goygFQ1c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raulespz/fastai_chest_x-ray_classify/blob/main/Fast_ai_Classifying_chest_X_Ray_Images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IONQ9hnKfcSN"
      },
      "source": [
        "#-------Import Dependencies-------#\n",
        "import pandas as pd\n",
        "import os,shutil,math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix,roc_curve,auc\n",
        "\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from skimage.io import imread\n",
        "from IPython.display import SVG\n",
        "\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.applications.vgg19 import VGG19,preprocess_input\n",
        "from keras.applications.xception import Xception\n",
        "from keras.applications.nasnet import NASNetMobile\n",
        "from keras.models import Sequential,Input,Model\n",
        "from keras.layers import Dense,Flatten,Dropout,Concatenate,GlobalAveragePooling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam,SGD\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping,TensorBoard,CSVLogger,ReduceLROnPlateau,LearningRateScheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBCSs3SRfz2K"
      },
      "source": [
        "#-----In case you want to use a learning rate scheduler from keras this is a good step decay function to play around with-----#\n",
        "def step_decay(epoch):\n",
        "    initial_lrate=0.1\n",
        "    drop=0.6\n",
        "    epochs_drop = 3.0\n",
        "    lrate= initial_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop))\n",
        "    return lrate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQaACSR7t3yQ"
      },
      "source": [
        "#----Custom function to visualize the training of the model------#\n",
        "def show_final_history(history):\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
        "    ax[0].set_title('loss')\n",
        "    ax[0].plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
        "    ax[0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n",
        "    ax[1].set_title('acc')\n",
        "    ax[1].plot(history.epoch, history.history[\"acc\"], label=\"Train acc\")\n",
        "    ax[1].plot(history.epoch, history.history[\"val_acc\"], label=\"Validation acc\")\n",
        "    ax[0].legend()\n",
        "    ax[1].legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pm_9SLG-Rrg"
      },
      "source": [
        "TRAINING_LOGS_FILE = \"training_logs.csv\"\n",
        "MODEL_SUMMARY_FILE = \"model_summary.txt\"\n",
        "MODEL_FILE = \"histopathologic_cancer_detector.h5\"\n",
        "TRAINING_PLOT_FILE = \"training.png\"\n",
        "VALIDATION_PLOT_FILE = \"validation.png\"\n",
        "ROC_PLOT_FILE = \"roc.png\"\n",
        "KAGGLE_SUBMISSION_FILE = \"kaggle_submission.csv\"\n",
        "INPUT_DIR = '../input/'\n",
        "SAMPLE_COUNT = 60000\n",
        "TESTING_BATCH_SIZE = 5000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "U7LynzbG-UOx",
        "outputId": "c6d491d4-ddaf-4b67-c38d-e38bc89ff298"
      },
      "source": [
        "training_dir = INPUT_DIR + 'train/'\n",
        "\n",
        "df = pd.DataFrame({'path': glob(os.path.join(training_dir,'*.tif'))})\n",
        "\n",
        "df['id'] = df.path.map(lambda x: x.split('/')[3].split('.')[0])\n",
        "\n",
        "labels = pd.read_csv(INPUT_DIR + 'train_labels.csv')\n",
        "\n",
        "df = df.merge(labels,on='id')\n",
        "\n",
        "negative_values = df[df.label == 0].sample(SAMPLE_COUNT)\n",
        "positive_values = df[df.label == 1].sample(SAMPLE_COUNT)\n",
        "\n",
        "df = pd.concat([negative_values,positive_values]).reset_index()\n",
        "\n",
        "df = df[['path','id','label']]\n",
        "df['image'] = df['path'].map(imread)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-4ad109e00f80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'train_labels.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/train_labels.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDjhe-Fv-UMW"
      },
      "source": [
        "train_path = '../training'\n",
        "val_path = '../validation'\n",
        "\n",
        "for directory in [train_path,val_path]:\n",
        "    for sub_directory in ['0','1']:\n",
        "        path = os.path.join(directory,sub_directory)\n",
        "        os.makedirs(path,exist_ok=True)\n",
        "\n",
        "train,val = train_test_split(df,train_size=0.8,stratify=df['label'])\n",
        "df.set_index('id',inplace=True)\n",
        "\n",
        "for images_paths in [(train,train_path),(val,val_path)]:\n",
        "    images = images_paths[0]\n",
        "    path = images_paths[1]\n",
        "    for image in images['id'].values:\n",
        "        file_name = image + '.tif'\n",
        "        label = str(df.loc[image,'label'])\n",
        "        destination = os.path.join(path,label,file_name)\n",
        "        if not os.path.exists(destination):\n",
        "            source = os.path.join(INPUT_DIR + 'train',file_name)\n",
        "            shutil.copyfile(source,destination)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2osF99E-UKC"
      },
      "source": [
        "#------Generators------------#\n",
        "train_augs = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    rotation_range=90,\n",
        "    zoom_range=0.2,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.05,\n",
        "    channel_shift_range=0.1\n",
        ")\n",
        "\n",
        "\n",
        "val_augs = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_gen = train_augs.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=(96,96),\n",
        "    batch_size=10,\n",
        "    class_mode='binary')\n",
        "\n",
        "val_gen = val_augs.flow_from_directory(\n",
        "    val_path,\n",
        "    target_size=(96,96),\n",
        "    batch_size=10,\n",
        "    class_mode='binary')\n",
        "print(train_gen.class_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgEwKoam-UG8"
      },
      "source": [
        "base_model = VGG19(include_top=False,\n",
        "                  input_shape = (96,96,3),\n",
        "                  weights = 'imagenet')\n",
        "\n",
        "for layer in base_model.layers[:-15]:\n",
        "    layer.trainable = False\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    print(layer,layer.trainable)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True, expand_nested=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlJx-rv4-UDv"
      },
      "source": [
        "#-------Callbacks-------------#\n",
        "checkpoint = ModelCheckpoint(\n",
        "    './base.model',\n",
        "    monitor='val_loss',\n",
        "    verbose=1,\n",
        "    save_best_only=True,\n",
        "    mode='min',\n",
        "    save_weights_only=False,\n",
        "    period=1\n",
        ")\n",
        "earlystop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    min_delta=0.001,\n",
        "    patience=10,\n",
        "    verbose=1,\n",
        "    mode='auto'\n",
        ")\n",
        "tensorboard = TensorBoard(\n",
        "    log_dir = './logs',\n",
        "    histogram_freq=0,\n",
        "    batch_size=16,\n",
        "    write_graph=True,\n",
        "    write_grads=True,\n",
        "    write_images=False,\n",
        ")\n",
        "\n",
        "csvlogger = CSVLogger(\n",
        "    filename= \"training_csv.log\",\n",
        "    separator = \",\",\n",
        "    append = False\n",
        ")\n",
        "\n",
        "#lrsched = LearningRateScheduler(step_decay,verbose=1)\n",
        "\n",
        "reduce = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.3,\n",
        "    patience=2,\n",
        "    verbose=1,\n",
        "    mode='auto',\n",
        "    cooldown=1\n",
        ")\n",
        "\n",
        "callbacks = [checkpoint,tensorboard,csvlogger,reduce]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsruJwsE-TuM"
      },
      "source": [
        "#------------Train The Model----------#\n",
        "#-------Use SGD with momentum and play with the learning rate and momentum----------#\n",
        "#------Good Momentum values: 0.9,0.99,0.5----------#\n",
        "opt = SGD(lr=1e-4,momentum=0.99)\n",
        "opt1 = Adam(lr=2e-4)\n",
        "\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=opt,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_gen,\n",
        "    steps_per_epoch  = 2000,\n",
        "    validation_data  = val_gen,\n",
        "    validation_steps = 2000,\n",
        "    epochs = 30,\n",
        "    verbose = 1,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbBiiakV-ToX"
      },
      "source": [
        "show_final_history(history)\n",
        "print(\"Validation Accuracy: \" + str(history.history['val_acc'][-1:]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiCLFvQI-8_u"
      },
      "source": [
        "roc_validation_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(val_path,\n",
        "                                                                                  target_size=(96,96),\n",
        "                                                                                  batch_size=10,\n",
        "                                                                                  class_mode='binary',\n",
        "                                                                                  shuffle=False)\n",
        "predictions = model.predict_generator(roc_validation_generator, steps=len(roc_validation_generator), verbose=1)\n",
        "false_positive_rate, true_positive_rate, threshold = roc_curve(roc_validation_generator.classes, predictions)\n",
        "area_under_curve = auc(false_positive_rate, true_positive_rate)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(false_positive_rate, true_positive_rate, label='AUC = {:.3f}'.format(area_under_curve))\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig(ROC_PLOT_FILE, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2TVBAvv-88o"
      },
      "source": [
        "testing_files = glob(os.path.join(INPUT_DIR+'test/','*.tif'))\n",
        "submission = pd.DataFrame()\n",
        "for index in range(0, len(testing_files), TESTING_BATCH_SIZE):\n",
        "    data_frame = pd.DataFrame({'path': testing_files[index:index+TESTING_BATCH_SIZE]})\n",
        "    data_frame['id'] = data_frame.path.map(lambda x: x.split('/')[3].split(\".\")[0])\n",
        "    data_frame['image'] = data_frame['path'].map(imread)\n",
        "    images = np.stack(data_frame.image, axis=0)\n",
        "    predicted_labels = [model.predict(np.expand_dims(image/255.0, axis=0))[0][0] for image in images]\n",
        "    predictions = np.array(predicted_labels)\n",
        "    data_frame['label'] = predictions\n",
        "    submission = pd.concat([submission, data_frame[[\"id\", \"label\"]]])\n",
        "submission.to_csv(KAGGLE_SUBMISSION_FILE, index=False, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILKWPK9l-8zf"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}